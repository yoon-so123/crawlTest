# # í…ìŠ¤íŠ¸ ì²˜ë¦¬
# import pandas as pd
# import re
# from collections import Counter

# # í˜•íƒœì†Œ ë¶„ì„ê¸° (ëª…ì‚¬ ì¶”ì¶œ)
# from konlpy.tag import Okt

# # ì‹œê°í™”
# import matplotlib.pyplot as plt
# from wordcloud import WordCloud
# # -----------------------------------------
# # 1. ë°ì´í„° ë¡œë”©
# # -----------------------------------------

# # ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
# df = pd.read_csv("dcinside_posts.csv")

# # ëª¨ë“  í…ìŠ¤íŠ¸ ê²°í•©
# all_text = df['title'].fillna('') + ' ' + df['content'].fillna('') + ' ' + df['comments'].fillna('')
# all_text = ' '.join(all_text.tolist())

# # í˜•íƒœì†Œ ë¶„ì„
# okt = Okt()
# nouns = okt.nouns(all_text)

# # í•œ ê¸€ì ì œì™¸í•˜ê³  ìƒìœ„ í‚¤ì›Œë“œ ì¶”ì¶œ
# filtered = [n for n in nouns if len(n) > 1]
# counter = Counter(filtered)
# top_keywords = counter.most_common(50)

# # ì¶œë ¥
# print("ğŸ“Œ ìƒìœ„ 50ê°œ í‚¤ì›Œë“œ:")
# for word, freq in top_keywords:
#     print(f"{word}: {freq}")
# # ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±


# import shutil
# import os

# # ê²½ë¡œ ì„¤ì • (data.py ê¸°ì¤€ìœ¼ë¡œ ìƒìœ„ í´ë”ë¡œ ì˜¬ë¼ê°)
# source_folder = '../genshin_notice_project/'
# target_folder = './'  # í˜„ì¬ í´ë”ì¸ crawlTest

# # ë³µì‚¬í•  íŒŒì¼ ëª©ë¡
# files_to_copy = ['ì›ì‹ _ì—…ë°ì´íŠ¸_í†µí•©ë³¸.csv', 'ì›ì‹ _ì´ë²¤íŠ¸_ë¦¬ìŠ¤íŠ¸_ì •ë¦¬ë³¸.csv']

# # íŒŒì¼ ë³µì‚¬ ì‹¤í–‰
# for file_name in files_to_copy:
#     src = os.path.join(source_folder, file_name)
#     dst = os.path.join(target_folder, file_name)
#     shutil.copy(src, dst)
#     print(f'{file_name} ë³µì‚¬ ì™„ë£Œ!')

# from selenium import webdriver
# from selenium.webdriver.chrome.options import Options
# from selenium.webdriver.common.by import By
# from bs4 import BeautifulSoup
# import time
# import csv

# # 1. í¬ë¡¬ ì˜µì…˜ ì„¤ì •
# options = Options()
# options.add_argument("--headless")
# options.add_argument("--no-sandbox")
# options.add_argument("--disable-dev-shm-usage")
# options.add_argument("user-agent=Mozilla/5.0")

# # 2. ë“œë¼ì´ë²„ ì‹¤í–‰
# driver = webdriver.Chrome(options=options)

# # 3. í¬ë¡¤ë§í•  ëª©ë¡ í˜ì´ì§€ URL
# base_url = "https://gall.dcinside.com/mgallery/board/lists/?id=onshinproject&page="
# start_page = 37
# num_posts_to_scrape = 50
# post_links = []

# # 4. ëª©ë¡ì—ì„œ ê²Œì‹œê¸€ ë§í¬ ìˆ˜ì§‘ (ì—¬ëŸ¬ í˜ì´ì§€ ëŒë©´ì„œ)
# while len(post_links) < num_posts_to_scrape:
#     driver.get(base_url + str(start_page))
#     time.sleep(2)
#     soup = BeautifulSoup(driver.page_source, 'html.parser')
#     rows = soup.select('tbody > tr.ub-content')
    
#     for row in rows:
#         a_tag = row.select_one('td.gall_tit > a[href]')
#         if a_tag and 'view' in a_tag['href']:
#             link = "https://gall.dcinside.com" + a_tag['href']
#             post_links.append(link)
#             if len(post_links) >= num_posts_to_scrape:
#                 break
#     start_page += 1

# print(f"âœ… {len(post_links)}ê°œì˜ ê²Œì‹œê¸€ ë§í¬ ìˆ˜ì§‘ ì™„ë£Œ")

# # 5. ê²Œì‹œê¸€ ë³¸ë¬¸+ëŒ“ê¸€ ì¶”ì¶œ
# results = []

# for idx, url in enumerate(post_links):
#     driver.get(url)
#     time.sleep(1)
#     soup = BeautifulSoup(driver.page_source, 'html.parser')

#     # ë³¸ë¬¸
#     content_area = soup.select_one('div.write_div')
#     content_text = content_area.get_text(strip=True) if content_area else "[ë³¸ë¬¸ ì—†ìŒ]"

#     # ëŒ“ê¸€
#     comments = []
#     comment_items = soup.select('ul.cmt_list > li.ub-content')
#     for li in comment_items:
#         text_tag = li.select_one('.usertxt')
#         if text_tag:
#             comments.append(text_tag.get_text(strip=True))
    
#     results.append({
#         'url': url,
#         'content': content_text,
#         'comments': " | ".join(comments) if comments else "[ëŒ“ê¸€ ì—†ìŒ]"
#     })

#     print(f"ğŸ“„ ({idx+1}) ê²Œì‹œê¸€ ìˆ˜ì§‘ ì™„ë£Œ")

# # 6. CSV ì €ì¥ (í•œê¸€ ì¸ì½”ë”© í¬í•¨)
# csv_filename = "ì›ì‹ _5.6_ìœ ì €ë°˜ì‘_í¬ë¡¤ë§.csv"
# with open(csv_filename, mode='w', encoding='utf-8-sig', newline='') as f:
#     writer = csv.DictWriter(f, fieldnames=['url', 'content', 'comments'])
#     writer.writeheader()
#     for item in results:
#         writer.writerow(item)

# print(f"\nâœ… CSV ì €ì¥ ì™„ë£Œ: {csv_filename}")

# # ë“œë¼ì´ë²„ ì¢…ë£Œ
# driver.quit()


from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from bs4 import BeautifulSoup
import time
import csv

# 1. í¬ë¡¬ ì˜µì…˜ ì„¤ì •
options = Options()
options.add_argument("--headless")
options.add_argument("--no-sandbox")
options.add_argument("--disable-dev-shm-usage")
options.add_argument("user-agent=Mozilla/5.0")

# 2. ë“œë¼ì´ë²„ ì‹¤í–‰
driver = webdriver.Chrome(options=options)

# 3. í¬ë¡¤ë§í•  ëª©ë¡ í˜ì´ì§€ URL
base_url = "https://gall.dcinside.com/mgallery/board/lists/?id=onshinproject&page="
start_page = 37
num_posts_to_scrape = 50
post_links = []

# 4. ëª©ë¡ì—ì„œ ì œëª© + ë§í¬ ìˆ˜ì§‘
while len(post_links) < num_posts_to_scrape:
    driver.get(base_url + str(start_page))
    time.sleep(2)
    soup = BeautifulSoup(driver.page_source, 'html.parser')
    rows = soup.select('tbody > tr.ub-content')
    
    for row in rows:
        a_tag = row.select_one('td.gall_tit > a[href]')
        if a_tag and 'view' in a_tag['href']:
            link = "https://gall.dcinside.com" + a_tag['href']
            title = a_tag.text.strip()
            post_links.append({
                'title': title,
                'url': link
            })
            if len(post_links) >= num_posts_to_scrape:
                break
    start_page += 1

print(f"âœ… {len(post_links)}ê°œì˜ ê²Œì‹œê¸€ ì œëª©+ë§í¬ ìˆ˜ì§‘ ì™„ë£Œ")

# 5. ê²Œì‹œê¸€ ë³¸ë¬¸+ëŒ“ê¸€ ìˆ˜ì§‘
results = []

for idx, post in enumerate(post_links):
    url = post['url']
    title = post['title']
    
    driver.get(url)
    time.sleep(1)
    soup = BeautifulSoup(driver.page_source, 'html.parser')

    # ë³¸ë¬¸
    content_area = soup.select_one('div.write_div')
    content_text = content_area.get_text(strip=True) if content_area else "[ë³¸ë¬¸ ì—†ìŒ]"

    # ëŒ“ê¸€
    comments = []
    comment_items = soup.select('ul.cmt_list > li.ub-content')
    for li in comment_items:
        text_tag = li.select_one('.usertxt')
        if text_tag:
            comments.append(text_tag.get_text(strip=True))
    
    results.append({
        'title': title,
        'content': content_text,
        'comments': " | ".join(comments) if comments else "[ëŒ“ê¸€ ì—†ìŒ]"
    })

    print(f"ğŸ“„ ({idx+1}) {title[:30]}... ìˆ˜ì§‘ ì™„ë£Œ")

# 6. CSV ì €ì¥
csv_filename = "ì›ì‹ _5.6_ìœ ì €ë°˜ì‘_í¬ë¡¤ë§.csv"
with open(csv_filename, mode='w', encoding='utf-8-sig', newline='') as f:
    writer = csv.DictWriter(f, fieldnames=['title', 'content', 'comments'])
    writer.writeheader()
    for item in results:
        writer.writerow(item)

print(f"\nâœ… CSV ì €ì¥ ì™„ë£Œ: {csv_filename}")


additional_links = []
start_page = 39  # ê¸°ì¡´ ë§ˆì§€ë§‰ í˜ì´ì§€ë³´ë‹¤ +1

while len(additional_links) < 50:
    driver.get(base_url + str(start_page))
    time.sleep(2)
    soup = BeautifulSoup(driver.page_source, 'html.parser')
    rows = soup.select('tbody > tr.ub-content')
    
    for row in rows:
        a_tag = row.select_one('td.gall_tit > a[href]')
        if a_tag and 'view' in a_tag['href']:
            link = "https://gall.dcinside.com" + a_tag['href']
            title = a_tag.text.strip()
            additional_links.append({
                'title': title,
                'url': link
            })
            if len(additional_links) >= 50:
                break
    start_page += 1

print(f"â• ì¶”ê°€ 50ê°œ ê²Œì‹œê¸€ ë§í¬ ìˆ˜ì§‘ ì™„ë£Œ")

# 50ê°œ ë” ë³¸ë¬¸/ëŒ“ê¸€ í¬ë¡¤ë§
for idx, post in enumerate(additional_links):
    url = post['url']
    title = post['title']
    
    driver.get(url)
    time.sleep(1)
    soup = BeautifulSoup(driver.page_source, 'html.parser')

    # ë³¸ë¬¸
    content_area = soup.select_one('div.write_div')
    content_text = content_area.get_text(strip=True) if content_area else "[ë³¸ë¬¸ ì—†ìŒ]"

    # ëŒ“ê¸€
    comments = []
    comment_items = soup.select('ul.cmt_list > li.ub-content')
    for li in comment_items:
        text_tag = li.select_one('.usertxt')
        if text_tag:
            comments.append(text_tag.get_text(strip=True))
    
    results.append({
        'title': title,
        'content': content_text,
        'comments': " | ".join(comments) if comments else "[ëŒ“ê¸€ ì—†ìŒ]"
    })

    print(f"ğŸ“„ ({idx+51}) ì¶”ê°€ ê²Œì‹œê¸€ ìˆ˜ì§‘ ì™„ë£Œ")

# ë‹¤ì‹œ CSV ì €ì¥
csv_filename = "ì›ì‹ _5.6_ìœ ì €ë°˜ì‘_í¬ë¡¤ë§_100ê°œ.csv"
with open(csv_filename, mode='w', encoding='utf-8-sig', newline='') as f:
    writer = csv.DictWriter(f, fieldnames=['title', 'content', 'comments'])
    writer.writeheader()
    for item in results:
        writer.writerow(item)

print(f"\nâœ… ì „ì²´ 100ê°œ CSV ì €ì¥ ì™„ë£Œ: {csv_filename}")
driver.quit()